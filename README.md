# Superstore Sales-prediction

Business Understanding:

Due to the fierce competition in the retail sector, businesses are using data analytics and machine learning to better understand the tastes and behaviour of their customers. To increase revenue, one strategy is to develop a model that predicts sales based on numerous product and outlet parameters. The model's performance will be assessed using performance measures like mean absolute error (MAE) and root mean square error (RMSE), Mean Squared Error (MSE), R2 score, and data-driven decisions will be made based on the insights gathered from the model to increase sales success and profitability.

Data Understanding:

The dataset used in this study contains information on products and their sales and was obtained from Kaggle.com. The structure and information of the dataset were investigated, and graphs were plotted to understand its distribution.

Inferences from the graphs:

•	From the ‘Item_outlet_sales’ to the ‘density graph’, we understand that the sales data is positively skewed. This shows a positive trend that the high value sales are less, and healthy sales amounts is between 0 and 2000.
•	From the ‘Item_visibility’ to the ‘density’ graph, we understand that there are multiple products falling under 0 visibility.
•	There are Low Fat and Regular fat products i.e. Two types in ‘Item_Fat_Content’ column.
•	Fruits and vegetables are the types of items which were sold maximum.
•	Most products MRP ranges between $75-125 or 140 -200.
•	Most of the stores are of supermarket type 1 and it has highest sales.
•	Tier 1 is doing well in sales in general, for all the products. However, soft drinks and Breakfast are the most sold items.
•	Sea food and starchy food makes the least sale in tier 2.
•	Less visible products are generating sales, but the sales generated by high visible products is more.
•	From the correlation heap map, we saw that the ‘Item_MRP’ and ‘Item_Outlet_Sales’ are somewhat correlated but it’s still not a strong/good correlation.


Data Preparation::

The dataset was checked for missing and duplicate values. The columns ‘Item_Weight’ and ‘Outlet_size’ had missing values which were imputed using mean and mode respectively. Outliers were found in ‘Item_Outlet_Sales’ and ‘Item_Visibility’ and were treated using the interquartile range and median and some of them were dropped as well. No duplicates were found.

Feature Engineering:

Outliers were still present after treatment and were not further treated to avoid wrong prediction. The ‘Item_Fat_Content’ column was corrected for incorrect labels and categorical columns were converted to numerical.
Also, the categorical columns were changed to numeric using ‘LabelEncoder’ and all the features were scaled using MinMaxScaler.

Modelling and Pipelines creation:

We split the dataset into train and test data using ‘train_test_split’ function and models were employed without and with Ensembling Technique (Bagging and Boosting).
1.Linear Regression.
•	Linear Regression without ensembling techniques.
We plotted the scatter plot diagram for the test and the predicted values, and we saw that the points are closer to the diagonal line which makes it a good model for the sales prediction.
•	When comparing the Mean squared error, root mean squared error, mean absolute error and R-squared score of the model with and without ensembling, we saw that there is no much difference in values. So, we can use the simple linear regression model here.
2.Decision Tree Regressor
•	When comparing the model with the ensemble technique and the non-ensemble one, the ensemble model performed better than the non-ensemble model with a higher R-squared score and lower error metrics, indicating better overall performance.
•	In the scatter plot diagram for the same, the decision tree with the ensemble technique is closer to the diagonal line.
3.Random Forest Regressor
•	The use of ensemble techniques improved the performance of the random forest model with lower error metrics and higher R-squared score. Ensemble techniques such as Random Forest or Gradient Boosting are recommended for regression tasks with decision trees to improve accuracy and robustness.
In the scatter plot diagram, the random forest with ensemble technique is more effective at forecasting sales values as its points are clustered more closely around the diagonal line than the random forest without ensemble technique.

Hyperparameter Tuning (Using GridSearchCV)

1.	Decision Tree Model
The decision tree model with hyperparameter tuning performed better than the one without tuning with a higher R-squared score and lower error metrics. The decision tree bagging model with hyperparameter tuning outperformed the other DT models as it has the highest R-squared score and lowest errors in predicting sales.


2.	Random Forest Model
Overall, the tuned Random Forest with ensembling performed the best among other Random Forest models with the highest R-squared score and lowest error metrics. The Random Forest without ensembling had a lower R-squared score while the Random Forest with ensembling had a slightly higher score than the tuned version.
3.	XGBoost	
The XGBoost model with hyperparameter tuning outperformed the model without tuning with a higher R-squared score and lower error metrics, indicating better performance in predicting sales.

Model Evaluation and Selection:

The models with the highest scores were
1. XGBoost with hyperparameter tuning,2. Decision Tree Model with Bagging (with hyperparameter tuning), 3. Random Forest Regressor (with hyperparameter tuning), and 4. Linear Regression with Bagging Regressor.

Conclusion:

The Random Forest Regressor with Hyperparameter Tuning seems to perform better than the other three models. It has lower MSE, RMSE, and MAE values and a higher R-squared score of 0.56. This suggests that the Random Forest Regressor should be the best model for this dataset.

